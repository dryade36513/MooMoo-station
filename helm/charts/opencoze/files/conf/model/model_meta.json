{
    "provider2models": {
        "Claude": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 131072
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "5",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            }        
        },
        "GPT": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 400000
                },
                "capability": {
                    "cot_display": false,
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false,
                    "prefill_resp": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "4096",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    }
                ]
            }
        },
        "Gemini": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 204800
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": true,
                    "audio_understanding": true,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "Temperature",
                        "desc": "**Temperature**:\n\n- When you increase this value, the model outputs more diverse and innovative content; when you decrease it, the model outputs less diverse content that strictly follows the given instructions.\n- It is recommended not to adjust this value with \"Top p\" at the same time.",
                        "type": 1,
                        "min": "0",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "1",
                            "creative": "0.8",
                            "balance": "0.5",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top p",
                        "desc": "**Top P**:\n\n- An alternative to sampling with temperature, where only tokens within the top p probability mass are considered. For example, 0.1 means only the top 10% probability mass tokens are considered.\n- We recommend altering this or temperature, but not both.",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.94",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "Generation diversity"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "Response max length",
                        "desc": "You can specify the maximum length of the tokens output through this value. Typically, 100 tokens are approximately equal to 150 Chinese characters.",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "Output format",
                        "desc": "**Output Format**:\n\n- **Text**: Replies in plain text format\n- **Markdown**: Uses Markdown format for replies\n- **JSON**: Uses JSON format for replies",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "Text",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "Input and output settings"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "Current time",
                        "desc": "The current accurate time will be appended to each user query after enabled. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "Prompt leakage prevention",
                        "desc": "The system prompt will be reinforced after enabled, which can significantly reduce the probability of system prompt leakage. [GuideDoc](https://www.coze.com/open/docs/guides/llm#3a97d6f3)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "Default instruction"
                        }
                    }
                ]
            }
        },
        "QWen": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成隨機性",
                        "desc": "- **temperature**: 調高溫度會使得模型的輸出更多樣性和創新性，反之，降低溫度會使輸出內容更加遵循指令要求但減少多樣性。建議不要與“Top p”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1.99",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 爲累計概率**: 模型在生成輸出時會從概率最高的詞彙開始選擇，直到這些詞彙的總概率累積達到Top p 值。這樣可以限制模型只選擇這些高概率的詞彙，從而控制輸出內容的多樣性。建議不要與“生成隨機性”同時調整。",
                        "type": 1,
                        "min": "0.01",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.8",
                            "creative": "0.8",
                            "balance": "0.8",
                            "precise": "0.8"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    },
                    {
                        "name": "response_format",
                        "label": "輸出格式",
                        "desc": "- **文本**: 使用普通文本格式回覆\n- **Markdown**: 將引導模型使用Markdown格式輸出回覆\n- **JSON**: 將引導模型使用JSON格式輸出",
                        "type": 2,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [
                            {
                                "label": "文本",
                                "value": "0"
                            },
                            {
                                "label": "Markdown",
                                "value": "1"
                            }
                        ],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    }
                ]
            }
        },
        "SEED": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 229376
                },
                "capability": {
                    "cot_display": true,
                    "function_call": true,
                    "image_understanding": true,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": true
                },
                "parameters": [
                    {
                        "name": "top_p",
                        "label": "Top P",
                        "desc": "- **Top p 爲累計概率**: 模型在生成輸出時會從概率最高的詞彙開始選擇，直到這些詞彙的總概率累積達到Top p 值。這樣可以限制模型只選擇這些高概率的詞彙，從而控制輸出內容的多樣性。建議不要與“生成隨機性”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.7",
                            "creative": "1",
                            "balance": "1",
                            "precise": "1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "frequency_penalty",
                        "label": "重複語句懲罰",
                        "desc": "- **frequency penalty**: 當該值爲正時，會阻止模型頻繁使用相同的詞彙和短語，從而增加輸出內容的多樣性。",
                        "type": 1,
                        "min": "-2",
                        "max": "2",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0",
                            "creative": "0",
                            "balance": "0",
                            "precise": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "0",
                        "max": "32768",
                        "precision": 0,
                        "default_val": {
                            "default_val": "4096"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    },
                    {
                        "name": "max_completion_tokens",
                        "label": "最大推理&回答長度",
                        "desc": "控制模型思維鏈推理和回覆輸出的最大長度（單位 token）。配置了該參數後，可以讓模型輸出超長內容，max_tokens （最大回復長度，默認值 4k）與思維鏈最大長度將失效，模型按需輸出內容，直到達到“最大推理&回覆長度”（max_completion_tokens） 配置的值。\n注意：若與“最大回復長度”（max_tokens） 字段同時設置，則“最大回復長度”不會生效。",
                        "type": 2,
                        "min": "0",
                        "max": "65536",
                        "precision": 0,
                        "default_val": {
                            "default_val": "0"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    }
                ]
            }
        },
        "Llama": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成隨機性",
                        "desc": "- **temperature**: 調高溫度會使得模型的輸出更多樣性和創新性，反之，降低溫度會使輸出內容更加遵循指令要求但減少多樣性。建議不要與“Top p”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    }
                ]
            }
        },
        "DeekSeek": {
            "default": {
                "display_info": {
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成隨機性",
                        "desc": "- **temperature**: 調高溫度會使得模型的輸出更多樣性和創新性，反之，降低溫度會使輸出內容更加遵循指令要求但減少多樣性。建議不要與“Top p”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 2,
                        "default_val": {
                            "default_val": "0.85",
                            "creative": "0.95",
                            "balance": "0.85",
                            "precise": "0.1"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "5",
                        "max": "2000",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2000"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    }
                ]
            },
            "deepseek-reasoner": {
                "display_info": {
                    "name": "DeepSeek-R1·工具調用",
                    "description": {
                        "zh_cn": "R1 functionCall 版本，支持在Single-Agent模式下調用各類釦子工具（插件、工作流、知識庫等）。",
                        "en_us": "DeepSeek-R1 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 8192
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成隨機性",
                        "desc": "- **temperature**: 調高溫度會使得模型的輸出更多樣性和創新性，反之，降低溫度會使輸出內容更加遵循指令要求但減少多樣性。建議不要與“Top p”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "2200"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "當前時間",
                        "desc": "開啓後，會在用戶的每次query中拼上當前準確時間。[指引文檔](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默認指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "開啓後將會加固提示詞，顯著降低提示詞泄露情況的出現概率。[指引文檔](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默認指令"
                        }
                    }
                ]
            },
            "deepseek-chat": {
                "display_info": {
                    "name": "DeepSeek-V3·工具調用",
                    "description": {
                        "zh_cn": "V3 functionCall 版本，支持在Single-Agent模式下調用各類釦子工具（插件、工作流、知識庫等）。",
                        "en_us": "DeepSeek-V3 functionCall version, which supports calling various Coze tools (plugins, workflows, knowledge bases, etc.) in Single-Agent mode."
                    },
                    "output_tokens": 4096,
                    "max_tokens": 65536
                },
                "capability": {
                    "function_call": true,
                    "image_understanding": false,
                    "video_understanding": false,
                    "audio_understanding": false,
                    "support_multi_modal": false
                },
                "parameters": [
                    {
                        "name": "temperature",
                        "label": "生成隨機性",
                        "desc": "- **temperature**: 調高溫度會使得模型的輸出更多樣性和創新性，反之，降低溫度會使輸出內容更加遵循指令要求但減少多樣性。建議不要與“Top p”同時調整。",
                        "type": 1,
                        "min": "0",
                        "max": "1",
                        "precision": 1,
                        "default_val": {
                            "default_val": "1",
                            "creative": "1",
                            "balance": "0.8",
                            "precise": "0.3"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 1,
                            "label": "生成多樣性"
                        }
                    },
                    {
                        "name": "max_tokens",
                        "label": "最大回復長度",
                        "desc": "控制模型輸出的Tokens 長度上限。通常 100 Tokens 約等於 150 箇中文漢字。",
                        "type": 2,
                        "min": "1",
                        "max": "8192",
                        "precision": 0,
                        "default_val": {
                            "default_val": "1024"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 2,
                            "label": "輸入及輸出設置"
                        }
                    },
                    {
                        "name": "sp_current_time",
                        "label": "當前時間",
                        "desc": "開啓後，會在用戶的每次query中拼上當前準確時間。[指引文檔](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默認指令"
                        }
                    },
                    {
                        "name": "sp_anti_leak",
                        "label": "SP防泄漏指令",
                        "desc": "開啓後將會加固提示詞，顯著降低提示詞泄露情況的出現概率。[指引文檔](http://coze.cn/open/docs/guides/llm#79e75604)",
                        "type": 3,
                        "min": "",
                        "max": "",
                        "precision": 0,
                        "default_val": {
                            "default_val": "false"
                        },
                        "options": [],
                        "param_class": {
                            "class_id": 5,
                            "label": "模型默認指令"
                        }
                    }
                ]
            }
        }
    }
}